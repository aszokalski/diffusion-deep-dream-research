{
 "cells": [
  {
   "metadata": {
    "id": "1632cf80cbc5ebcf"
   },
   "cell_type": "markdown",
   "source": [
    "# 03: Entangled Deep Dream"
   ],
   "id": "1632cf80cbc5ebcf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "https://colab.research.google.com/drive/1rWvCRiu0eZZ1zjm_KuVqcaJxkob9TBDl?usp=sharing",
   "id": "8bf58a6b8d83a665"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b4072fb45adcf559",
    "outputId": "e8c68cbc-fa1e-47cb-a821-1ef2b6841768"
   },
   "cell_type": "code",
   "source": [
    "%pip install torch numpy matplotlib diffusers datasets tqdm transformers accelerate"
   ],
   "id": "b4072fb45adcf559",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "id": "initial_id"
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from diffusers import PNDMScheduler, UNet2DConditionModel, StableDiffusionPipeline\n",
    "from datasets import load_dataset\n",
    "from functools import lru_cache, cached_property\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "import pickle\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "10f32390c874eed7"
   },
   "cell_type": "markdown",
   "source": [
    "## Model setup"
   ],
   "id": "10f32390c874eed7"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "f2f88163cb7898a2",
    "outputId": "b28a015b-a852-4a8a-b28e-4e61fabb682c"
   },
   "cell_type": "code",
   "source": [
    "backend_name = \"cuda:0\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "backend_name"
   ],
   "id": "f2f88163cb7898a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "8d133cdffd616122"
   },
   "cell_type": "code",
   "source": [
    "dtype = torch.float32\n",
    "if backend_name == \"cuda:0\":\n",
    "    dtype = torch.float16"
   ],
   "id": "8d133cdffd616122",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188,
     "referenced_widgets": [
      "7aafd129893e4a9593fbc322fe9e260e",
      "73373273d134426483249e77f9436f28",
      "8d64fedd7a294863816b0421e38c0301",
      "4e19f6f266584cedb797b8aacda9a9e0",
      "643f82056ba34b5783df2461863440d1",
      "d0fd47c5405048d38f40bb55822fc693",
      "95dabec5cf934f419989734115d7ef3d",
      "758bb46fbb8f48eda481b7b1a1113a05",
      "6c38f16e9baa4231897a575a3e0bd523",
      "fb142c9e04844325acc6bffd66e1bde3",
      "681004b27ec44243a631efcf503e4645"
     ]
    },
    "id": "e36a046bdb15d4f7",
    "outputId": "80e3902a-5358-4e94-a1c8-ee1fbcf3e9ed"
   },
   "cell_type": "code",
   "source": [
    "model_id = \"sd-legacy/stable-diffusion-v1-5\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=dtype)\n",
    "pipe = pipe.to(backend_name)"
   ],
   "id": "e36a046bdb15d4f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "7868ebcf4b849db7"
   },
   "cell_type": "markdown",
   "source": [
    "## Test data"
   ],
   "id": "7868ebcf4b849db7"
  },
  {
   "metadata": {
    "id": "3b7b164d25d38cf1"
   },
   "cell_type": "code",
   "source": [
    "dataset = load_dataset(\"timm/mini-imagenet\", split=\"test\")"
   ],
   "id": "3b7b164d25d38cf1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "8a42ed4fb56e36ce",
    "outputId": "d4cd9738-82f2-4382-c81b-ca6c5cd3cccc"
   },
   "cell_type": "code",
   "source": [
    "test_img = dataset[300]['image']\n",
    "np_img = np.array(test_img)\n",
    "plt.axis('off')\n",
    "plt.imshow(np_img)"
   ],
   "id": "8a42ed4fb56e36ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3c811b62adbd65c4",
    "outputId": "910b3775-0401-4099-cf72-4fc2d8dfd958"
   },
   "cell_type": "code",
   "source": [
    "test_images = torch.tensor(np.array([np_img] * 5), device=pipe.device, dtype=dtype) # (N, H, W, C)\n",
    "\n",
    "def to_chw(images: torch.Tensor) -> torch.Tensor:\n",
    "    return images.permute(0, 3, 1, 2)  # (N, H, W, C) -> (N, C, H, W)\n",
    "\n",
    "def to_hwc(images: torch.Tensor) -> torch.Tensor:\n",
    "    return images.permute(0, 2, 3, 1)  # (N, C, H, W) -> (N, H, W, C)\n",
    "\n",
    "def normalize(tensor: torch.Tensor) -> torch.Tensor:\n",
    "    max_val = tensor.max()\n",
    "    min_val = tensor.min()\n",
    "\n",
    "    if max_val == min_val:\n",
    "        return torch.zeros_like(tensor)\n",
    "\n",
    "    norm_0_1 = (tensor - min_val) / (max_val - min_val)\n",
    "\n",
    "    return norm_0_1 * 2.0 - 1.0\n",
    "\n",
    "def denormalize(tensor: torch.Tensor) -> torch.Tensor:\n",
    "    max_val = tensor.max()\n",
    "    min_val = tensor.min()\n",
    "\n",
    "    if max_val == min_val:\n",
    "        return torch.zeros_like(tensor)\n",
    "    tensor = (tensor - min_val) / (max_val - min_val)\n",
    "\n",
    "    return tensor * 255.0\n",
    "\n",
    "test_images = to_chw(test_images)\n",
    "\n",
    "test_images = torch.nn.functional.interpolate(test_images, size=(128, 128), mode='bilinear')\n",
    "\n",
    "test_images = normalize(test_images)\n",
    "\n",
    "def show_images(images: torch.Tensor, size=(15, 15), title=None, save_path=None) -> torch.Tensor:\n",
    "    images = denormalize(images)\n",
    "\n",
    "    plt.figure(figsize=size)\n",
    "\n",
    "    if title is not None:\n",
    "        plt.suptitle(title, fontsize=16, y=0.6)\n",
    "\n",
    "    for i in range(images.shape[0]):\n",
    "        img = to_hwc(images)[i].cpu().int().numpy()\n",
    "        plt.subplot(1, images.shape[0], i + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "\n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path, bbox_inches='tight', dpi=150)\n",
    "        print(f\"Figure saved to {save_path}\")\n",
    "\n",
    "    plt.show()\n",
    "    return images\n",
    "\n",
    "show_images(test_images, title=\"test title\")"
   ],
   "id": "3c811b62adbd65c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "e8142b1357256e0a"
   },
   "cell_type": "markdown",
   "source": [
    "## Model wrapper"
   ],
   "id": "e8142b1357256e0a"
  },
  {
   "metadata": {
    "id": "575d21ff471e4d0d"
   },
   "cell_type": "code",
   "source": [
    "class UNetWrapper(nn.Module):\n",
    "    def __init__(self, pipe: StableDiffusionPipeline, timestep: int, prompt: str = \"\", use_noise: bool = True):\n",
    "        super().__init__()\n",
    "        self.pipe = pipe\n",
    "        self.unet: UNet2DConditionModel = pipe.unet\n",
    "        self.scheduler: PNDMScheduler = pipe.scheduler\n",
    "        self.device = pipe.device\n",
    "        self.dtype = pipe.unet.dtype\n",
    "        self.prompt = prompt\n",
    "        self.timestep = timestep\n",
    "        self.use_noise = use_noise\n",
    "\n",
    "    @lru_cache\n",
    "    def embeddings(self, batch_size: int):\n",
    "        with torch.no_grad():\n",
    "            _, negative_embeds = self.pipe.encode_prompt(\n",
    "                prompt=\"\",\n",
    "                device=self.device,\n",
    "                num_images_per_prompt=batch_size,\n",
    "                do_classifier_free_guidance=True,\n",
    "                negative_prompt=None\n",
    "            )\n",
    "        return negative_embeds\n",
    "\n",
    "    @lru_cache\n",
    "    def timestep_tensor(self, timestep):\n",
    "        return torch.tensor([timestep], device=self.device, dtype=self.dtype).int()\n",
    "\n",
    "\n",
    "    def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the UNet with the given timestep and prompt embeddings.\n",
    "        :param z: Latent input tensor of shape (batch_size, channels, height, width)\n",
    "        :return: Latent output tensor of shape (batch_size, channels, height, width)\n",
    "        \"\"\"\n",
    "        z = z.to(self.device, dtype=self.dtype)\n",
    "\n",
    "        batch_size = z.shape[0]\n",
    "        embeddings = self.embeddings(batch_size)\n",
    "        c = z.shape[1]\n",
    "        h = z.shape[2]\n",
    "        w = z.shape[3]\n",
    "        if self.use_noise:\n",
    "            noise = torch.randn((batch_size, c, h, w), device=pipe.device, dtype=pipe.dtype)\n",
    "            z_noise = self.scheduler.add_noise(z, noise, self.timestep_tensor(self.timestep))\n",
    "\n",
    "        return self.unet(\n",
    "            sample=z_noise if self.use_noise else z,\n",
    "            timestep=self.timestep,\n",
    "            encoder_hidden_states=embeddings\n",
    "        ).sample"
   ],
   "id": "575d21ff471e4d0d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "5e209d41fd682954"
   },
   "cell_type": "code",
   "source": [
    "def encode_images(images: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Encode images into latent space using the VAE encoder.\n",
    "    :param images: Input images tensor of shape (batch_size, channels, height, width)\n",
    "    :return: Latent representation tensor of shape (batch_size, latent_channels, latent_height, latent_width)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        latent_dist = pipe.vae.encode(images).latent_dist\n",
    "        latents = latent_dist.sample()\n",
    "        latents = latents * pipe.vae.config.scaling_factor\n",
    "    return latents\n",
    "\n",
    "def decode_latents(latents: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Decode latent representations back into images using the VAE decoder.\n",
    "    :param latents: Latent representation tensor of shape (batch_size, latent_channels, latent_height, latent_width)\n",
    "    :return: Decoded images tensor of shape (batch_size, channels, height, width)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        latents = latents / pipe.vae.config.scaling_factor\n",
    "        images = pipe.vae.decode(latents.to(pipe.vae.dtype)).sample\n",
    "    return images"
   ],
   "id": "5e209d41fd682954",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ad9b6f3682fbfb5",
    "outputId": "8744d43b-37b5-4f03-ec24-2954cf3e7b62"
   },
   "cell_type": "code",
   "source": [
    "model_test = UNetWrapper(pipe, timestep=1).to(backend_name).eval()\n",
    "print(test_images.shape)\n",
    "latent = encode_images(test_images)\n",
    "print(latent.shape)\n",
    "latent_noise = model_test.forward(latent)\n",
    "print(latent_noise.shape)\n",
    "noise = decode_latents(latent_noise)\n",
    "print(noise.shape)"
   ],
   "id": "ad9b6f3682fbfb5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "31da655b4ebeaad6",
    "outputId": "6a46be3c-e393-43d2-e82d-9961667db254"
   },
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    show_images(noise)"
   ],
   "id": "31da655b4ebeaad6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "e9383a0a888fc0dd"
   },
   "cell_type": "markdown",
   "source": [
    "## Deep Dream sanity check\n",
    "First we will do a deep dream of a layer in the UNet without any additional noising and SAE"
   ],
   "id": "e9383a0a888fc0dd"
  },
  {
   "metadata": {
    "id": "9570bf23035013db"
   },
   "cell_type": "markdown",
   "source": [
    "### Choosing a layer"
   ],
   "id": "9570bf23035013db"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "35a32d5021d618f3",
    "outputId": "5d8456d1-51e4-4054-f37e-49f52f7172fb"
   },
   "cell_type": "code",
   "source": [
    "module_names = []\n",
    "for module in model_test.unet.named_modules():\n",
    "    print(module[0])\n",
    "    module_names.append(module[0])"
   ],
   "id": "35a32d5021d618f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "id": "fc649be046ce5c77"
   },
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class E0Config:\n",
    "    layer_name: str\n",
    "    channel: int\n",
    "    latent_size: int\n",
    "    batch_size: int\n",
    "    timestep: int\n",
    "\n",
    "    @cached_property\n",
    "    def layer_depth(self) -> float:\n",
    "        \"\"\"\n",
    "        Returns the depth of the layer as a percentage of the total number of layers [0, 1].\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        block = self.layer_name.split(\".\")[0]\n",
    "        if block == \"down_blocks\":\n",
    "            return 0.2\n",
    "        elif block == \"mid_blocks\":\n",
    "            return 0.6\n",
    "        else:\n",
    "            return 0.8\n",
    "\n",
    "\n",
    "    @cached_property\n",
    "    def steps(self) -> int:\n",
    "        return math.floor(50 + 250 * self.layer_depth)\n",
    "\n",
    "    @cached_property\n",
    "    def lr(self) -> float:\n",
    "        return 0.1 ** (self.layer_depth + 1)\n",
    "\n",
    "\n",
    "def experiment_e0(config: E0Config, model: UNetWrapper, use_noise = True) -> torch.Tensor:\n",
    "    model.timestep = config.timestep\n",
    "    model.use_noise = use_noise\n",
    "\n",
    "    #float32 used here for both cuda, mps and cpu\n",
    "    latents = torch.randn(config.batch_size, 4, config.latent_size, config.latent_size, device=model.device, dtype=torch.float32) * 0.01\n",
    "    latents.requires_grad_(True)\n",
    "\n",
    "    optimizer = torch.optim.Adam([latents], lr=config.lr)\n",
    "\n",
    "    scaler = GradScaler(enabled=(dtype == torch.float16))\n",
    "\n",
    "    activations = {}\n",
    "    def get_activation(name):\n",
    "        def hook(model, input, output):\n",
    "            # for attention layers\n",
    "            if isinstance(output, tuple):\n",
    "                activations[name] = output[0]\n",
    "            else:\n",
    "                activations[name] = output\n",
    "        return hook\n",
    "\n",
    "    target_layer = dict(model.unet.named_modules())[config.layer_name]\n",
    "    hook_handle = target_layer.register_forward_hook(get_activation(\"target\"))\n",
    "\n",
    "    print(f\"Optimizing Channel {config.channel} on {config.layer_name} at timestep {config.timestep} for {config.steps} steps with lr {config.lr}...\")\n",
    "\n",
    "\n",
    "    pbar = tqdm(range(config.steps))\n",
    "    try:\n",
    "        for i in pbar:\n",
    "            activations.clear()\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            with autocast(enabled=(dtype == torch.float16)):\n",
    "                _ = model(latents)\n",
    "\n",
    "                if \"target\" not in activations:\n",
    "                    raise RuntimeError(f\"Hook failed to trigger on step {i}. The layer '{config.layer_name}' might be skipped in the forward pass.\")\n",
    "\n",
    "                act = activations[\"target\"]\n",
    "\n",
    "                loss = -act[:, config.channel].mean()\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Optimization stopped by user.\")\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "    finally:\n",
    "        hook_handle.remove()\n",
    "        print(\"Hook removed.\")\n",
    "\n",
    "    print(\"Decoding final result...\")\n",
    "    final_images = decode_latents(latents.detach())\n",
    "    return final_images"
   ],
   "id": "fc649be046ce5c77",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "configs = []\n",
    "\n",
    "for l in [\"down_blocks.1.resnets.1.conv1\", \"down_blocks.1.resnets.0.conv1\", \"down_blocks.0.attentions.1.transformer_blocks.0.attn1\"]:\n",
    "    for c in range(0, 8):\n",
    "        for t in [0, 50, 200, 400, 800, 999]:\n",
    "            configs.append(E0Config(layer_name=l, channel=c, latent_size=16, batch_size=5, timestep=t))\n",
    "\n",
    "os.makedirs(\"./results\", exist_ok=True)\n",
    "pickle_path = \"./results/experiment_results.pkl\"\n",
    "\n",
    "results = {}\n",
    "for i, config in enumerate(configs):\n",
    "        print(f\"EXPERIMENT {i+1}/{len(configs)}\")\n",
    "        print(config)\n",
    "        final_image = experiment_e0(config, model_test)\n",
    "        title = f\"Layer: {config.layer_name}, Channel: {config.channel}, Timestep: {config.timestep}, Noise: True\"\n",
    "        key=f\"{config.layer_name}-ch{config.channel}-t{config.timestep}-it{config.steps}-lr{config.lr}\"\n",
    "        path = f\"./results/{key}.png\"\n",
    "        result = show_images(final_image, size=(20,20), title=title, save_path=path)\n",
    "        results[key] = result\n",
    "\n",
    "        final_image = experiment_e0(config, model_test, use_noise=False)\n",
    "        title = f\"Layer: {config.layer_name}, Channel: {config.channel}, Timestep: {config.timestep}, Noise: False\"\n",
    "        key=f\"{config.layer_name}-ch{config.channel}-t{config.timestep}-it{config.steps}-lr{config.lr}-no-noise\"\n",
    "        path = f\"./results/{key}.png\"\n",
    "        result = show_images(final_image, size=(20,20), title=title, save_path=path)\n",
    "        results[key] = result\n",
    "\n",
    "        # Incremental Save Pickle\n",
    "        try:\n",
    "            with open(pickle_path, \"wb\") as f:\n",
    "                pickle.dump(results, f)\n",
    "            print(f\"Progress saved to {pickle_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving pickle: {e}\")"
   ],
   "metadata": {
    "id": "0e6junsx3sB4",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "c7a8ee18-f95d-4908-f51f-05471909c64d"
   },
   "id": "0e6junsx3sB4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# !zip -r results.zip results/",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4T4n08LoVNlF",
    "outputId": "19de07d8-2c71-45a8-9681-5b566ab578dd"
   },
   "id": "4T4n08LoVNlF",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "zpasW1CnsY7t",
    "outputId": "192b8cb1-d65f-4129-bcc3-0e771b4af7a6"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# from google.colab import files\n",
    "# files.download(\"results.zip\")"
   ],
   "id": "zpasW1CnsY7t"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "gpuType": "A100",
   "machine_shape": "hm"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "7aafd129893e4a9593fbc322fe9e260e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_73373273d134426483249e77f9436f28",
       "IPY_MODEL_8d64fedd7a294863816b0421e38c0301",
       "IPY_MODEL_4e19f6f266584cedb797b8aacda9a9e0"
      ],
      "layout": "IPY_MODEL_643f82056ba34b5783df2461863440d1"
     }
    },
    "73373273d134426483249e77f9436f28": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d0fd47c5405048d38f40bb55822fc693",
      "placeholder": "​",
      "style": "IPY_MODEL_95dabec5cf934f419989734115d7ef3d",
      "value": "Loading pipeline components...: 100%"
     }
    },
    "8d64fedd7a294863816b0421e38c0301": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_758bb46fbb8f48eda481b7b1a1113a05",
      "max": 7,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6c38f16e9baa4231897a575a3e0bd523",
      "value": 7
     }
    },
    "4e19f6f266584cedb797b8aacda9a9e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fb142c9e04844325acc6bffd66e1bde3",
      "placeholder": "​",
      "style": "IPY_MODEL_681004b27ec44243a631efcf503e4645",
      "value": " 7/7 [00:01&lt;00:00,  4.52it/s]"
     }
    },
    "643f82056ba34b5783df2461863440d1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d0fd47c5405048d38f40bb55822fc693": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "95dabec5cf934f419989734115d7ef3d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "758bb46fbb8f48eda481b7b1a1113a05": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6c38f16e9baa4231897a575a3e0bd523": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fb142c9e04844325acc6bffd66e1bde3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "681004b27ec44243a631efcf503e4645": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
